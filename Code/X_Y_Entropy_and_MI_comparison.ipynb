{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeds for reproducibility\n",
    "from numpy.random import seed\n",
    "seed(1337)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1337)\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import binning\n",
    "import npeet\n",
    "from EDGE.EDGE_4_3_1 import EDGE\n",
    "import KDE\n",
    "import data_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook calculates the entropy of input and output and the mutual information between them for the \"Opening the Blackbox\" and MNIST dataset using different estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tishby Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tishby Data...\n"
     ]
    }
   ],
   "source": [
    "set_name = \"tishby\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = data_selection.select_data(set_name, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### since all samples are different the entropy of the input has to be the same as the logarith base 2 of the amount of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input data 3276\n",
      "Logarithm of amount of input data 11.67771964164101\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of input data\", len(X_train))\n",
    "print(\"Logarithm of amount of input data\",math.log(len(X_train),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binning estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin MI(X,Y) 0.9976734295143714\n",
      "Bin H(X) 11.677719641641012\n"
     ]
    }
   ],
   "source": [
    "unique_inverse_x, unique_inverse_y, px, py = binning.extract_inout_probs(X_train,\n",
    "                                                                           y_train)\n",
    "IX_Y, HX = binning.calc_information_between_in_out(px, py, X_train, y_train, unique_inverse_x,\n",
    "                                                   unique_inverse_y, False)\n",
    "print(\"Bin MI(X,Y)\", IX_Y)\n",
    "print(\"Bin H(X)\", HX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KSG estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KSG MI(X,Y) 0.9976734295143643\n",
      "KSG H(X) 11.67771964164101\n"
     ]
    }
   ],
   "source": [
    "print(\"KSG MI(X,Y)\",npeet.midd(X_train.tolist(), y_train.tolist()))\n",
    "print(\"KSG H(X)\",npeet.entropyd(X_train.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EDGE estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDGE MI(X,Y) 0.19461987597998315\n"
     ]
    }
   ],
   "source": [
    "print(\"EDGE MI(X,Y)\", EDGE(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KDE estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper H(X) -23.55183904416382\n",
      "Upper H(X|Y) -24.549512473678192\n",
      "Upper MI(X,Y) 0.99767342951437\n",
      "Lower H(X) -23.551839044163817\n",
      "Lower H(X|Y) -24.54951247367818\n",
      "Lower MI(X,Y) 0.9976734295143649\n"
     ]
    }
   ],
   "source": [
    "def KDEt(KDE_estimator_func, name):\n",
    "    labelprobs = np.mean(y_train, axis=0)\n",
    "    label_indices = {}\n",
    "    for i in range(y_train.shape[1]):\n",
    "         # return make non categorical to extract indices\n",
    "         # from: https://github.com/keras-team/keras/issues/4981\n",
    "         label_indices[i] = np.argmax(y_train, axis=1) == i\n",
    "\n",
    "    entropy_X = KDE_estimator_func(X_train,noise_variance)[0]\n",
    "    entropy_X_giv_Y = 0.\n",
    "    for i in label_indices.keys():\n",
    "        entropy_cond = KDE_estimator_func(X_train[label_indices[i],:],noise_variance)[0]\n",
    "        entropy_X_giv_Y += labelprobs[i] * entropy_cond\n",
    "        \n",
    "    print(name+\" H(X)\", nats2bits * entropy_X)\n",
    "    print(name+\" H(X|Y)\", nats2bits * entropy_X_giv_Y)\n",
    "    print(name+\" MI(X,Y)\", nats2bits * (entropy_X - entropy_X_giv_Y))\n",
    "\n",
    "noise_variance = 1e-3\n",
    "nats2bits = 1.0/np.log(2)\n",
    "\n",
    "KDEt(KDE.entropy_estimator_kl, \"Upper\")\n",
    "KDEt(KDE.entropy_estimator_bd, \"Lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test entropy of [1, 1] (upper) -1.935796557150402\n",
      "Test entropy of [1, 1] (lower) -1.9357965571504028\n"
     ]
    }
   ],
   "source": [
    "# Check entropy function results for KDE\n",
    "# result should be 0.5\n",
    "TestEntropyH = KDE.entropy_estimator_kl(np.array([[1], [2]]), noise_variance)[0]\n",
    "print(\"Test entropy of [1, 1] (upper)\", nats2bits * TestEntropyH)\n",
    "TestEntropyL = KDE.entropy_estimator_bd(np.array([[1], [2]]), noise_variance)[0]\n",
    "print(\"Test entropy of [1, 1] (lower)\", nats2bits * TestEntropyL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mnist Data...\n",
      "X_train shape  (3000, 28, 28)\n",
      "y_train shape  (3000,)\n"
     ]
    }
   ],
   "source": [
    "set_name = \"mnist\"\n",
    "\n",
    "nrs = [3,8,1]\n",
    "samples = 1000\n",
    "X_train, X_test, y_train, y_test = data_selection.select_data(set_name, shuffle=False,\n",
    "                                                              samples_per_class = samples,\n",
    "                                                              list_of_nrs=nrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input data 3000\n",
      "Logarithm of amount of input data 11.550746785383243\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of input data\", len(X_train))\n",
    "print(\"Logarithm of amount of input data\",math.log(len(X_train),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binning estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bin MI(X,Y) 1.5849625007211543\n",
      "Bin H(X) 11.550746785383241\n"
     ]
    }
   ],
   "source": [
    "unique_inverse_x, unique_inverse_y, px, py = binning.extract_inout_probs(X_train,\n",
    "                                                                           y_train)\n",
    "IX_Y, HX = binning.calc_information_between_in_out(px, py, X_train, y_train, unique_inverse_x,\n",
    "                                                   unique_inverse_y, False)\n",
    "print(\"Bin MI(X,Y)\", IX_Y)\n",
    "print(\"Bin H(X)\", HX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KSG estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KSG MI(X,Y) 1.584962500721156\n",
      "KSG H(X) 11.55074678538324\n"
     ]
    }
   ],
   "source": [
    "print(\"KSG MI(X,Y)\",npeet.midd(X_train.tolist(), y_train.tolist()))\n",
    "print(\"KSG H(X)\",npeet.entropyd(X_train.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EDGE estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDGE 0.2936402363927715\n"
     ]
    }
   ],
   "source": [
    "print(\"EDGE\", EDGE(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KDE estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper H(X) -2290.107902704249\n",
      "Upper H(X|Y) -2291.692610891892\n",
      "Upper MI(X,Y) 1.5847081876426452\n",
      "Lower H(X) -2290.1119841060085\n",
      "Lower H(X|Y) -2291.6970445142524\n",
      "Lower MI(X,Y) 1.5850604082438087\n"
     ]
    }
   ],
   "source": [
    "def KDEt(KDE_estimator_func, name):\n",
    "    labelprobs = np.mean(y_train, axis=0)\n",
    "    label_indices = {}\n",
    "    for i in range(y_train.shape[1]):\n",
    "         # return make non categorical to extract indices\n",
    "         # from: https://github.com/keras-team/keras/issues/4981\n",
    "         label_indices[i] = np.argmax(y_train, axis=1) == i\n",
    "\n",
    "    entropy_X = KDE_estimator_func(X_train,noise_variance)[0]\n",
    "    entropy_X_giv_Y = 0.\n",
    "    for i in label_indices.keys():\n",
    "        entropy_cond = KDE_estimator_func(X_train[label_indices[i],:],noise_variance)[0]\n",
    "        entropy_X_giv_Y += labelprobs[i] * entropy_cond\n",
    "        \n",
    "    print(name+\" H(X)\", nats2bits * entropy_X)\n",
    "    print(name+\" H(X|Y)\", nats2bits * entropy_X_giv_Y)\n",
    "    print(name+\" MI(X,Y)\", nats2bits * (entropy_X - entropy_X_giv_Y))\n",
    "\n",
    "noise_variance = 1e-3\n",
    "nats2bits = 1.0/np.log(2)\n",
    "\n",
    "KDEt(KDE.entropy_estimator_kl, \"Upper\")\n",
    "KDEt(KDE.entropy_estimator_bd, \"Lower\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
